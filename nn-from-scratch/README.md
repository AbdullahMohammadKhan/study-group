# NN from scratch

The purpose here was to  write a neural network "from scratch", which is to say without using any of the available libraries. The advantage being deeper understanding of the principles and how they work, the disadvantages being performance, versatility and effort.

This nn incorporates most of the features we've dealt with so far in the course (that is, up to somewhere in week 3): cross entropy, L2 regularization, and improved weight initialization.

The to do list:
- Create more versatility in terms of number of layers, number of neurons per layer
- Implement some form on minibatching? Is this practical / logical given the optimizer is doing the?