# Deep Learning Study Group

We are a diverse group of software engineering and data science professionals from industry and academia. 

Inspired by the [collaborative culture of artificial neural network research](https://www.untapt.com/industry/2016/08/02/deep-learning-study-group/), we hold regular, chilled beverage-enhanced study sessions in midtown Manhattan. At these meetings, we summarise prescribed preparatory material and leverage our individual strengths in computer science, mathematics, statistics, neuroscience, and venture capital to cement our comprehension of concepts and to implement effective deep learning models. 

Over the course of our sessions, we follow three parallel paths: 

1. **Theory**: We study academic textbooks, exercises, and coursework so that we command strong theoretical foundations for neural networks and deep learning. Broadly, we cover calculus, algebra, probability, computer science, with a focus on their intersection at machine learning. 
2. **Application**: We practice deep learning in the real world. We typically commence by collectively following tutorials then we move on to solving novel and illustrative data problems involving a broad range of techniques. In addition to incorporating deep learning into our respective academic and commercial applications, we commit code to the present, public repository where possible. 
3. **Presentations**: Study group members regularly share their progress on Deep Learning projects and their area of expertise. This elicits novel discourse outside of the relatively formal paths **1** and **2**, playfully encouraging along serendipity. 

## Theory

Thus far, we have covered: 

1. Michael Nielsen's introductory text [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) (covered in *sessions I through V*)
2. Fei-Fei Li, Andrej Karpathy and Justin Johnson's [CS231n on Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/) (*sessions VI through VIII*)
3. Richard Socher's [CS224d on Deep Learning for Natural Language Processing](https://cs224d.stanford.edu/) (*sessions IX through XIV*)


## Application

Our applications have largely involved convolutional neural networks built in Python with [Numpy](https://github.com/the-deep-learners/study-group/tree/master/nn-from-scratch), [TensorFlow, or Keras](https://insights.untapt.com/fundamental-deep-learning-code-in-tflearn-keras-theano-and-tensorflow-66be10a03227). 


## Presentations

In chronological order, we have experienced the joy of being enlightened by: 

1. [Dr. Katya Vasilaky](https://kathrynthegreat.github.io/): on the mathematics of deep learning (*session II*) and on regularization (*session VIII*)
2. [Dr. Thomas Balestri](https://www.linkedin.com/in/thomasbalestri): on countless machine-learning underpinnings (*sessions III and IV*)
3. [Gabe Rives-Corbett](https://www.linkedin.com/in/grivescorbett): on Keras implementations of deep learning deployed at [untapt](https://www.untapt.com/) (*session 3*)
2. [Dmitri Nesterenko](https://github.com/dmitrinesterenko): on his NumPy implementation of k-Nearest Neighbours (*session VI*)
3. [Raphaela Sapire](https://www.linkedin.com/in/raphaelasapire): on the deep learning start-up investment atmosphere (*session VIII*)


# Session Notes

See the [weekly work](https://github.com/the-deep-learners/study-group/tree/master/weekly-work) subdirectories for details of what we covered each session, with summary notes where they were deemed necessary. We provide updates after each of our sessions, i.e., approximately every third week. 


*With a desire to remain intimately-sized, our study group has reached its capacity. If you'd like to be added to our waiting list, please contact the organiser, [Dr. Jon Krohn](https://www.jonkrohn.com/contact/), describing your relevant experience as well as your interest in deep learning. We don't expect you to be a deep learning expert already :)*
